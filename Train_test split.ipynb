{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpyCyi6msMcp",
        "outputId": "c0561b0f-4d8f-424d-d958-8fb7d6f59728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Major-project'...\n",
            "remote: Enumerating objects: 5738, done.\u001b[K\n",
            "remote: Counting objects: 100% (479/479), done.\u001b[K\n",
            "remote: Compressing objects: 100% (476/476), done.\u001b[K\n",
            "remote: Total 5738 (delta 4), reused 446 (delta 3), pack-reused 5259\u001b[K\n",
            "Receiving objects: 100% (5738/5738), 665.02 MiB | 25.18 MiB/s, done.\n",
            "Resolving deltas: 100% (19/19), done.\n",
            "Updating files: 100% (5638/5638), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nabin25/Major-project"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install  tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcbJ9SZts7hb",
        "outputId": "7e96d7ed-9628-451b-caba-d4cf6d8a8c54"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amYRWoC0tHI8",
        "outputId": "9a750e07-98f0-427b-813c-e86b578efdab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct 21 14:47:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__\n",
        "!pip install opencv-python\n",
        "!pip install mediapipe\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "import os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "My3YgnbdtSHP",
        "outputId": "7b5011d8-e3de-4107-d36b-17286d5f2a3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n",
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.4.6-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.9 sounddevice-0.4.6\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ba\n"
      ],
      "metadata": {
        "id": "EYM8wnHExogr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Ba\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ba-ब\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.2)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnJbbNWZxp9L",
        "outputId": "31724391-409f-4d38-beb8-653eec9930c6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 102\n",
            "Validation set size: 26\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0099.png', 'hand_0027.png', 'hand_0121.png', 'hand_0041.png', 'hand_0003.png']\n",
            "Validation set: ['hand_0044.png', 'hand_0001.png', 'hand_0037.png', 'hand_0148.png', 'hand_0008.png']\n",
            "Testing set: ['hand_0106.png', 'hand_0010.png', 'hand_0091.png', 'hand_0136.png', 'hand_0108.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import random"
      ],
      "metadata": {
        "id": "iK76v0KC0zqr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bha"
      ],
      "metadata": {
        "id": "6BM3V1L-bMLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Bha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-bha-भ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.2)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ee_mEpjgbWPx",
        "outputId": "c30c1743-4098-4cdc-8cba-adf82b669ea4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 100\n",
            "Validation set size: 25\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0116.png', 'hand_0072.png', 'hand_0079.png', 'hand_0032.png', 'hand_0117.png']\n",
            "Validation set: ['hand_0127.png', 'hand_0090.png', 'hand_0083.png', 'hand_0115.png', 'hand_0043.png']\n",
            "Testing set: ['hand_0018.png', 'hand_0138.png', 'hand_0027.png', 'hand_0094.png', 'hand_0010.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cha"
      ],
      "metadata": {
        "id": "wO36NA3ybwLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"cha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-cha-च\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIuk_hHObxv1",
        "outputId": "78a99516-f6ae-463c-aa3f-c0f3f7a8084e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 101\n",
            "Validation set size: 34\n",
            "Test set size: 34\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0130.png', 'hand_0079.png', 'hand_0003.png', 'hand_0039.png', 'hand_0036.png']\n",
            "Validation set: ['hand_0026.png', 'hand_0031.png', 'hand_0013.png', 'hand_0021.png', 'hand_0081.png']\n",
            "Testing set: ['hand_0019.png', 'hand_0142.png', 'hand_0063.png', 'hand_0025.png', 'hand_0056.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "chha\n"
      ],
      "metadata": {
        "id": "98p_c61lb916"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"chha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-chha-छ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvS78-hOb_6r",
        "outputId": "aae48f58-5fc8-4da1-b83f-b1990527d6e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 105\n",
            "Validation set size: 35\n",
            "Test set size: 35\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0025.png', 'hand_0004.png', 'hand_0158.png', 'hand_0058.png', 'hand_0144.png']\n",
            "Validation set: ['hand_0048.png', 'hand_0041.png', 'hand_0027.png', 'hand_0033.png', 'hand_0163.png']\n",
            "Testing set: ['hand_0040.png', 'hand_0011.png', 'hand_0089.png', 'hand_0166.png', 'hand_0034.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "d-sha"
      ],
      "metadata": {
        "id": "E0OhW10OcHIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Qoezpxmv3HMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"d-sha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-d_sha-स\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw8YA4itcLlG",
        "outputId": "8d9495c8-5b39-436b-d08b-ad03610988df"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 84\n",
            "Validation set size: 28\n",
            "Test set size: 28\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0066.png', 'hand_0074.png', 'hand_0052.png', 'hand_0086.png', 'hand_0124.png']\n",
            "Validation set: ['hand_0048.png', 'hand_0105.png', 'hand_0062.png', 'hand_0070.png', 'hand_0024.png']\n",
            "Testing set: ['hand_0005.png', 'hand_0116.png', 'hand_0059.png', 'hand_0111.png', 'hand_0007.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Da\n"
      ],
      "metadata": {
        "id": "D139JtZechjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Da\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-da-ड\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAVTy7W8ckir",
        "outputId": "f353a5db-ac30-4943-9b70-775253a981ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 111\n",
            "Validation set size: 38\n",
            "Test set size: 38\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0026.png', 'hand_0063.png', 'hand_0013.png', 'hand_0087.png', 'hand_0121.png']\n",
            "Validation set: ['hand_0096.png', 'hand_0114.png', 'hand_0047.png', 'hand_0054.png', 'hand_0084.png']\n",
            "Testing set: ['hand_0059.png', 'hand_0149.png', 'hand_0077.png', 'hand_0050.png', 'hand_0140.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daa"
      ],
      "metadata": {
        "id": "eRGIb7zMCG8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Daa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-daa-द\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mlh0ybrqCWJy",
        "outputId": "d68e97a5-89cf-4e67-98dd-be87e45b07f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 112\n",
            "Validation set size: 38\n",
            "Test set size: 38\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0135.png', 'hand_0093.png', 'hand_0028.png', 'hand_0180.png', 'hand_0155.png']\n",
            "Validation set: ['hand_0170.png', 'hand_0182.png', 'hand_0012.png', 'hand_0119.png', 'hand_0140.png']\n",
            "Testing set: ['hand_0046.png', 'hand_0005.png', 'hand_0096.png', 'hand_0139.png', 'hand_0162.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dha"
      ],
      "metadata": {
        "id": "YhtNkuMMChW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Dha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-dha-ढ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JuW9qu9jCpNF",
        "outputId": "1ec86998-9d89-4985-8e7c-5c6d68ea8f7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 120\n",
            "Validation set size: 40\n",
            "Test set size: 40\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0168.png', 'hand_0051.png', 'hand_0159.png', 'hand_0005.png', 'hand_0035.png']\n",
            "Validation set: ['hand_0194.png', 'hand_0060.png', 'hand_0085.png', 'hand_0165.png', 'hand_0021.png']\n",
            "Testing set: ['hand_0135.png', 'hand_0083.png', 'hand_0055.png', 'hand_0107.png', 'hand_0012.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dhaa"
      ],
      "metadata": {
        "id": "n_IMO_aDC6e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"Dhaa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-dhaa-ध\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "66xq4fHzC9Nl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869c62ed-d545-4ccf-b32a-2135f3b8396b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 95\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0124.png', 'hand_0058.png', 'hand_0097.png', 'hand_0062.png', 'hand_0033.png']\n",
            "Validation set: ['hand_0054.png', 'hand_0090.png', 'hand_0039.png', 'hand_0028.png', 'hand_0053.png']\n",
            "Testing set: ['hand_0121.png', 'hand_0057.png', 'hand_0015.png', 'hand_0041.png', 'hand_0149.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ga\n"
      ],
      "metadata": {
        "id": "4VYXpsr7DGMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ga\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ga-ग\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "DOf-5C-bDIDU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9cf06d0-3d2c-4ec2-88ef-724e78719398"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 72\n",
            "Validation set size: 24\n",
            "Test set size: 25\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0089.png', 'hand_0103.png', 'hand_0079.png', 'hand_0093.png', 'hand_0007.png']\n",
            "Validation set: ['hand_0067.png', 'hand_0113.png', 'hand_0099.png', 'hand_0049.png', 'hand_0012.png']\n",
            "Testing set: ['hand_0106.png', 'hand_0046.png', 'hand_0002.png', 'hand_0116.png', 'hand_0084.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gha"
      ],
      "metadata": {
        "id": "ESP0_rdIDJVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"gha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-gha-घ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "axtlUMuhDKSZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98f9b0c5-449a-4082-a94e-e2b04e0d559e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 107\n",
            "Validation set size: 36\n",
            "Test set size: 36\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0048.png', 'hand_0120.png', 'hand_0007.png', 'hand_0164.png', 'hand_0129.png']\n",
            "Validation set: ['hand_0100.png', 'hand_0027.png', 'hand_0124.png', 'hand_0079.png', 'hand_0151.png']\n",
            "Testing set: ['hand_0066.png', 'hand_0105.png', 'hand_0090.png', 'hand_0071.png', 'hand_0062.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gya"
      ],
      "metadata": {
        "id": "h2tA-DWlDLcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"gya\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-gya-ज्ञ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "TlO7GAd3DMkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b496b7-7ca4-4539-ecac-41493a11f684"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 93\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0140.png', 'hand_0148.png', 'hand_0119.png', 'hand_0014.png', 'hand_0127.png']\n",
            "Validation set: ['hand_0004.png', 'hand_0083.png', 'hand_0056.png', 'hand_0011.png', 'hand_0035.png']\n",
            "Testing set: ['hand_0043.png', 'hand_0109.png', 'hand_0110.png', 'hand_0013.png', 'hand_0094.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ha"
      ],
      "metadata": {
        "id": "L88LFNNdDOSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ha-ह\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "dpVBRvP2DMvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855b6436-6ffe-4e3c-e49d-210254de3430"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 75\n",
            "Validation set size: 26\n",
            "Test set size: 26\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0108.png', 'hand_0077.png', 'hand_0032.png', 'hand_0096.png', 'hand_0069.png']\n",
            "Validation set: ['hand_0030.png', 'hand_0073.png', 'hand_0094.png', 'hand_0070.png', 'hand_0101.png']\n",
            "Testing set: ['hand_0050.png', 'hand_0027.png', 'hand_0076.png', 'hand_0056.png', 'hand_0018.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ja"
      ],
      "metadata": {
        "id": "0EhT5rzyDTGs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ja\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ja-ज\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "BgjviSMIDUVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c6d1c7-9cd0-4768-b248-b88539797758"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 92\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0051.png', 'hand_0033.png', 'hand_0145.png', 'hand_0114.png', 'hand_0064.png']\n",
            "Validation set: ['hand_0116.png', 'hand_0006.png', 'hand_0091.png', 'hand_0044.png', 'hand_0001.png']\n",
            "Testing set: ['hand_0142.png', 'hand_0096.png', 'hand_0125.png', 'hand_0090.png', 'hand_0121.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "jha"
      ],
      "metadata": {
        "id": "nYTHv8jwDVbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"jha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-jha-झ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "9YF77vRQDWxR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d45876-f067-4457-8e10-677ebd3e68c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 85\n",
            "Validation set size: 29\n",
            "Test set size: 29\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0139.png', 'hand_0057.png', 'hand_0117.png', 'hand_0106.png', 'hand_0120.png']\n",
            "Validation set: ['hand_0045.png', 'hand_0037.png', 'hand_0141.png', 'hand_0088.png', 'hand_0004.png']\n",
            "Testing set: ['hand_0126.png', 'hand_0125.png', 'hand_0127.png', 'hand_0075.png', 'hand_0053.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ka"
      ],
      "metadata": {
        "id": "FCHk96LwDYM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ka\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ka-क\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "c91ZeccGDaYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0767a8-b7c4-41b9-80ee-635b3bcb3ed1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 90\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0009.png', 'hand_0081.png', 'hand_0012.png', 'hand_0084.png', 'hand_0059.png']\n",
            "Validation set: ['hand_0142.png', 'hand_0024.png', 'hand_0106.png', 'hand_0075.png', 'hand_0077.png']\n",
            "Testing set: ['hand_0150.png', 'hand_0083.png', 'hand_0139.png', 'hand_0098.png', 'hand_0087.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "kha\n"
      ],
      "metadata": {
        "id": "P4xpMBK_De7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"kha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-kha-ख\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "kv4YmEtoDgr1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "719f4c16-9681-4dfd-a859-c54fbd2c8492"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 99\n",
            "Validation set size: 33\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0152.png', 'hand_0110.png', 'hand_0112.png', 'hand_0014.png', 'hand_0096.png']\n",
            "Validation set: ['hand_0125.png', 'hand_0146.png', 'hand_0140.png', 'hand_0092.png', 'hand_0048.png']\n",
            "Testing set: ['hand_0164.png', 'hand_0115.png', 'hand_0051.png', 'hand_0106.png', 'hand_0049.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ksha\n"
      ],
      "metadata": {
        "id": "6o2VT7tODh25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ksha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ksha-क्ष\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "JtLQusDlDjqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15322273-fa74-4ac3-cd89-7da451fc4c81"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 87\n",
            "Validation set size: 29\n",
            "Test set size: 29\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0041.png', 'hand_0088.png', 'hand_0098.png', 'hand_0022.png', 'hand_0087.png']\n",
            "Validation set: ['hand_0135.png', 'hand_0110.png', 'hand_0086.png', 'hand_0018.png', 'hand_0139.png']\n",
            "Testing set: ['hand_0047.png', 'hand_0025.png', 'hand_0071.png', 'hand_0058.png', 'hand_0106.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "la"
      ],
      "metadata": {
        "id": "G4UNRbJ2Dk5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"la\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-la-ल\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "KJzESkn4Dm3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eed84f0-8b3d-4ad1-c7c0-6b13b6d48c4d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 92\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0150.png', 'hand_0026.png', 'hand_0047.png', 'hand_0092.png', 'hand_0100.png']\n",
            "Validation set: ['hand_0132.png', 'hand_0112.png', 'hand_0066.png', 'hand_0114.png', 'hand_0009.png']\n",
            "Testing set: ['hand_0101.png', 'hand_0118.png', 'hand_0069.png', 'hand_0051.png', 'hand_0143.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "m_sha"
      ],
      "metadata": {
        "id": "x9Vs6XI_Do6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"m_sha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-m_sha-ष\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "o97EGa60DqLS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a053705c-a4c2-4550-ddc9-52dad085cbea"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 82\n",
            "Validation set size: 28\n",
            "Test set size: 28\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0125.png', 'hand_0026.png', 'hand_0115.png', 'hand_0104.png', 'hand_0108.png']\n",
            "Validation set: ['hand_0095.png', 'hand_0033.png', 'hand_0051.png', 'hand_0008.png', 'hand_0039.png']\n",
            "Testing set: ['hand_0113.png', 'hand_0121.png', 'hand_0068.png', 'hand_0055.png', 'hand_0120.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ma"
      ],
      "metadata": {
        "id": "lspQFLv_Drer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ma\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ma-म\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "oDJuZhBVDsoc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "020a3eb5-a7f0-4d20-abfb-ad895a91b241"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 93\n",
            "Validation set size: 31\n",
            "Test set size: 31\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0115.png', 'hand_0152.png', 'hand_0006.png', 'hand_0098.png', 'hand_0002.png']\n",
            "Validation set: ['hand_0148.png', 'hand_0003.png', 'hand_0000.png', 'hand_0063.png', 'hand_0112.png']\n",
            "Testing set: ['hand_0125.png', 'hand_0045.png', 'hand_0080.png', 'hand_0153.png', 'hand_0055.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-na"
      ],
      "metadata": {
        "id": "oVUmauiTDt1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"na\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-na-ण\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "pHM8OAoKDv2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4005646-02f4-4506-cd43-58047658239b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 108\n",
            "Validation set size: 37\n",
            "Test set size: 37\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0159.png', 'hand_0155.png', 'hand_0125.png', 'hand_0010.png', 'hand_0141.png']\n",
            "Validation set: ['hand_0111.png', 'hand_0120.png', 'hand_0154.png', 'hand_0071.png', 'hand_0011.png']\n",
            "Testing set: ['hand_0170.png', 'hand_0068.png', 'hand_0000.png', 'hand_0118.png', 'hand_0078.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "naa"
      ],
      "metadata": {
        "id": "-zZs5_kZD4RQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"naa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-naa-न\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "lFnVy--fD5TQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2292a3d6-4cfb-4d26-fd41-ddc397d7a59d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 96\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0048.png', 'hand_0108.png', 'hand_0092.png', 'hand_0000.png', 'hand_0035.png']\n",
            "Validation set: ['hand_0120.png', 'hand_0018.png', 'hand_0078.png', 'hand_0019.png', 'hand_0141.png']\n",
            "Testing set: ['hand_0114.png', 'hand_0015.png', 'hand_0057.png', 'hand_0002.png', 'hand_0007.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nga"
      ],
      "metadata": {
        "id": "ri7bWtrBD74U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"nga\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-nga-ङ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "FPFu1DyWD-FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a21fe53-2e76-4903-bd49-d1f331464efa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 105\n",
            "Validation set size: 35\n",
            "Test set size: 35\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0089.png', 'hand_0110.png', 'hand_0145.png', 'hand_0151.png', 'hand_0162.png']\n",
            "Validation set: ['hand_0076.png', 'hand_0169.png', 'hand_0160.png', 'hand_0107.png', 'hand_0114.png']\n",
            "Testing set: ['hand_0032.png', 'hand_0140.png', 'hand_0116.png', 'hand_0100.png', 'hand_0026.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pa"
      ],
      "metadata": {
        "id": "m0ok2rdND_Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"pa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-pa-प\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "f9j8yATAEAuQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb20079-5896-4fbe-f81b-ecff04bbc645"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 62\n",
            "Validation set size: 21\n",
            "Test set size: 21\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0056.png', 'hand_0010.png', 'hand_0016.png', 'hand_0089.png', 'hand_0003.png']\n",
            "Validation set: ['hand_0052.png', 'hand_0049.png', 'hand_0006.png', 'hand_0081.png', 'hand_0065.png']\n",
            "Testing set: ['hand_0092.png', 'hand_0008.png', 'hand_0051.png', 'hand_0009.png', 'hand_0011.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pha"
      ],
      "metadata": {
        "id": "vSOH49lqEB8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"pha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-pha-फ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "WlbOO85aEEBH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddc2df1-d2eb-4d65-c5fe-0bcfcc83e450"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 87\n",
            "Validation set size: 29\n",
            "Test set size: 30\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0012.png', 'hand_0108.png', 'hand_0102.png', 'hand_0122.png', 'hand_0068.png']\n",
            "Validation set: ['hand_0121.png', 'hand_0065.png', 'hand_0048.png', 'hand_0018.png', 'hand_0075.png']\n",
            "Testing set: ['hand_0033.png', 'hand_0093.png', 'hand_0130.png', 'hand_0066.png', 'hand_0035.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ra"
      ],
      "metadata": {
        "id": "QmGGcydjEFfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ra\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ra-र\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "xYzGdxHaEIsb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f3b5fb7-3edd-4820-b9a0-77427e7f6645"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 99\n",
            "Validation set size: 33\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0135.png', 'hand_0137.png', 'hand_0113.png', 'hand_0122.png', 'hand_0124.png']\n",
            "Validation set: ['hand_0063.png', 'hand_0138.png', 'hand_0109.png', 'hand_0056.png', 'hand_0108.png']\n",
            "Testing set: ['hand_0087.png', 'hand_0149.png', 'hand_0153.png', 'hand_0074.png', 'hand_0001.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-t_sha"
      ],
      "metadata": {
        "id": "p7cTG4oPEJ0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"t_sha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-t_sha-श\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "sdLhRFv7EOwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc21f3f-d6ec-4fcb-b602-8fc4039217dd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 95\n",
            "Validation set size: 32\n",
            "Test set size: 32\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0150.png', 'hand_0093.png', 'hand_0022.png', 'hand_0139.png', 'hand_0147.png']\n",
            "Validation set: ['hand_0045.png', 'hand_0039.png', 'hand_0131.png', 'hand_0108.png', 'hand_0067.png']\n",
            "Testing set: ['hand_0019.png', 'hand_0138.png', 'hand_0069.png', 'hand_0121.png', 'hand_0009.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ta"
      ],
      "metadata": {
        "id": "inSMVz60ERhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ta\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ta-ट\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "UrN5gRlHESd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6080c63-eb85-4136-e786-8e78a3436e3a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 99\n",
            "Validation set size: 33\n",
            "Test set size: 33\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0106.png', 'hand_0144.png', 'hand_0098.png', 'hand_0020.png', 'hand_0076.png']\n",
            "Validation set: ['hand_0048.png', 'hand_0005.png', 'hand_0131.png', 'hand_0154.png', 'hand_0150.png']\n",
            "Testing set: ['hand_0148.png', 'hand_0108.png', 'hand_0060.png', 'hand_0002.png', 'hand_0136.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "taa"
      ],
      "metadata": {
        "id": "IYPaoisHEU9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"taa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-taa-त\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "9LRHYbNFEWFY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61787cc2-771d-4dcf-8a50-62eef213a2b7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 114\n",
            "Validation set size: 38\n",
            "Test set size: 38\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0013.png', 'hand_0126.png', 'hand_0055.png', 'hand_0136.png', 'hand_0169.png']\n",
            "Validation set: ['hand_0065.png', 'hand_0008.png', 'hand_0067.png', 'hand_0028.png', 'hand_0075.png']\n",
            "Testing set: ['hand_0174.png', 'hand_0106.png', 'hand_0093.png', 'hand_0189.png', 'hand_0037.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tha"
      ],
      "metadata": {
        "id": "-5YiNHi8EXId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"tha\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-tha-ठ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "JkvfM1rTEYz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ba39ab-a8fc-4b98-b7c8-879bf352530c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 102\n",
            "Validation set size: 35\n",
            "Test set size: 35\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0056.png', 'hand_0136.png', 'hand_0145.png', 'hand_0024.png', 'hand_0063.png']\n",
            "Validation set: ['hand_0050.png', 'hand_0075.png', 'hand_0106.png', 'hand_0054.png', 'hand_0090.png']\n",
            "Testing set: ['hand_0147.png', 'hand_0065.png', 'hand_0070.png', 'hand_0026.png', 'hand_0030.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "thaa"
      ],
      "metadata": {
        "id": "yUDZu-v3EZ0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"thaa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-thaa-थ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "YNT120vyEbL4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e96f70c-f110-4526-bf36-7929fcf2434f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 102\n",
            "Validation set size: 34\n",
            "Test set size: 34\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0112.png', 'hand_0137.png', 'hand_0132.png', 'hand_0096.png', 'hand_0119.png']\n",
            "Validation set: ['hand_0123.png', 'hand_0156.png', 'hand_0039.png', 'hand_0111.png', 'hand_0167.png']\n",
            "Testing set: ['hand_0011.png', 'hand_0110.png', 'hand_0104.png', 'hand_0130.png', 'hand_0090.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tra\n"
      ],
      "metadata": {
        "id": "C_OD3vocEcWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"tra\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-tra-त्र\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "gdXKUuAFEdpK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fcdd17b-bc37-4fb8-dec0-3eab1ac417fd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 90\n",
            "Validation set size: 30\n",
            "Test set size: 30\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0060.png', 'hand_0082.png', 'hand_0014.png', 'hand_0069.png', 'hand_0147.png']\n",
            "Validation set: ['hand_0054.png', 'hand_0024.png', 'hand_0131.png', 'hand_0020.png', 'hand_0071.png']\n",
            "Testing set: ['hand_0079.png', 'hand_0089.png', 'hand_0015.png', 'hand_0033.png', 'hand_0042.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wa"
      ],
      "metadata": {
        "id": "JdLlTNUKEfGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"wa\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-wa-व\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "KFn_jcx0EgWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34544b2-7498-4b82-db7a-afe901c8589d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 100\n",
            "Validation set size: 34\n",
            "Test set size: 34\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0082.png', 'hand_0154.png', 'hand_0134.png', 'hand_0117.png', 'hand_0156.png']\n",
            "Validation set: ['hand_0155.png', 'hand_0035.png', 'hand_0046.png', 'hand_0041.png', 'hand_0023.png']\n",
            "Testing set: ['hand_0139.png', 'hand_0003.png', 'hand_0113.png', 'hand_0020.png', 'hand_0103.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ya"
      ],
      "metadata": {
        "id": "qH_63NuFEhJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"ya\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-ya-य\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "dJI0hRscEjUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5030f685-0856-491b-ff28-f9265a42d742"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 27\n",
            "Validation set size: 9\n",
            "Test set size: 9\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0006.png', 'hand_0028.png', 'hand_0040.png', 'hand_0011.png', 'hand_0023.png']\n",
            "Validation set: ['hand_0030.png', 'hand_0033.png', 'hand_0017.png', 'hand_0000.png', 'hand_0013.png']\n",
            "Testing set: ['hand_0036.png', 'hand_0035.png', 'hand_0021.png', 'hand_0031.png', 'hand_0005.png']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "yan"
      ],
      "metadata": {
        "id": "xp3XAA5gEnYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path for image dataset \"yan\"\n",
        "dataset_dir =\"/content/Major-project/pre-images/pre-for-yan-ञ\"\n",
        "image_files = [file for file in os.listdir(dataset_dir) if file.endswith(\".png\")]\n",
        "#shuffle the list of images\n",
        "print(\"hand_0014\")\n",
        "random.shuffle(image_files)\n",
        "#split the dataset into training and test\n",
        "train_files, test_files = train_test_split(image_files, test_size=0.2)\n",
        "train_files, val_files = train_test_split(train_files, test_size=0.25)\n",
        "# Print sizes of each set\n",
        "print(f\"Train set size: {len(train_files)}\")\n",
        "print(f\"Validation set size: {len(val_files)}\")\n",
        "print(f\"Test set size: {len(test_files)}\")\n",
        "# Print example file names from each set\n",
        "print(\"\\nExample files from each set:\")\n",
        "print(\"Training set:\", train_files[:5])\n",
        "print(\"Validation set:\", val_files[:5])\n",
        "print(\"Testing set:\", test_files[:5])\n",
        "\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Create directories for train and test sets if they don't exist already\n",
        "train_dir = os.path.join(dataset_dir, \"train\")\n",
        "test_dir = os.path.join(dataset_dir, \"test\")\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "# Move training files to the train directory\n",
        "for file in train_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(train_dir, file)\n",
        "    shutil.move(src_path, dst_path)\n",
        "\n",
        "# Move testing files to the test directory\n",
        "for file in test_files:\n",
        "    src_path = os.path.join(dataset_dir, file)\n",
        "    dst_path = os.path.join(test_dir, file)\n",
        "    shutil.move(src_path, dst_path)"
      ],
      "metadata": {
        "id": "5tL-YBgkEokF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7434720e-91d0-4a57-e3a4-777e680e8cfa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hand_0014\n",
            "Train set size: 80\n",
            "Validation set size: 27\n",
            "Test set size: 27\n",
            "\n",
            "Example files from each set:\n",
            "Training set: ['hand_0028.png', 'hand_0014.png', 'hand_0109.png', 'hand_0089.png', 'hand_0130.png']\n",
            "Validation set: ['hand_0013.png', 'hand_0120.png', 'hand_0091.png', 'hand_0000.png', 'hand_0081.png']\n",
            "Testing set: ['hand_0124.png', 'hand_0071.png', 'hand_0066.png', 'hand_0072.png', 'hand_0131.png']\n"
          ]
        }
      ]
    }
  ]
}